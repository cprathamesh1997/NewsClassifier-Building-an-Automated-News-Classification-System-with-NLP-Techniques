{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NewsClassifier: Building an Automated News Classification System with NLP Techniques**"
      ],
      "metadata": {
        "id": "sO3jsUdE4P7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The objective of the NewsClassifier project is to develop an automated news classification system using natural language processing (NLP) techniques. The system aims to extract, preprocess, analyze, and categorize news articles from a popular news website. Key goals include grouping similar articles using K-means clustering and building a classification model to automatically categorize news articles.**\n"
      ],
      "metadata": {
        "id": "z_A5StUv37wJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ***So developed a Python script to extract, preprocess, and analyze news articles from a popular news website. The primary goals were to group similar articles using K-means clustering and build a classification model to automatically categorize news articles.***"
      ],
      "metadata": {
        "id": "Ypi4fGlr62jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Import required libraries\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "#Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    #Using BeautifulSoup\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    cleaned_text = soup.get_text(separator=' ')\n",
        "\n",
        "    #Removing non-alphabetic characters and extra whitespaces\n",
        "    cleaned_text = ' '.join(word for word in cleaned_text.split() if word.isalpha())\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    #Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    #Removing stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    #Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def extract_information(url):\n",
        "    #Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    #Checking if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "\n",
        "        #Parsing the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        #Extracting information from the \"fpj_bignews\" section\n",
        "        bignews_article = soup.find('div', class_='fpj_bignews')\n",
        "\n",
        "        if bignews_article:\n",
        "            bignews_title = bignews_article.find('h3').text.strip()\n",
        "\n",
        "            #Cleaning and preprocessing text\n",
        "            bignews_title_cleaned = clean_text(bignews_title)\n",
        "            bignews_tokens = tokenize_and_remove_stopwords(bignews_title_cleaned)\n",
        "            bignews_lemmatized_tokens = lemmatize_tokens(bignews_tokens)\n",
        "\n",
        "            #Text representation using TF-IDF\n",
        "            tfidf_vectorizer = TfidfVectorizer()\n",
        "            bignews_tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(bignews_lemmatized_tokens)])\n",
        "\n",
        "            #Printing information from the \"fpj_bignews\" section\n",
        "            print(f\"BIG NEWS - Title: {bignews_title}\")\n",
        "            print(f\"Cleaned Title: {bignews_title_cleaned}\")\n",
        "            print(f\"Lemmatized Tokens: {bignews_lemmatized_tokens}\")\n",
        "            print(f\"TF-IDF Matrix: {bignews_tfidf_matrix}\")\n",
        "            print()\n",
        "        else:\n",
        "            print(\"Big news section not found on the page.\")\n",
        "\n",
        "        #Extracting information from the \"fpj_newList\" section\n",
        "        newlist_articles = soup.find('div', class_='fpj_newList')\n",
        "\n",
        "        if newlist_articles:\n",
        "            newlist_articles = newlist_articles.find_all('li')\n",
        "\n",
        "            #List to store preprocessed text data for clustering\n",
        "            article_lemmatized_tokens_list = []\n",
        "\n",
        "            for article in newlist_articles:\n",
        "                article_title = article.find('span', class_='fpj_title').text.strip()\n",
        "\n",
        "                #Cleaning and preprocess text\n",
        "                article_title_cleaned = clean_text(article_title)\n",
        "                article_tokens = tokenize_and_remove_stopwords(article_title_cleaned)\n",
        "                article_lemmatized_tokens = lemmatize_tokens(article_tokens)\n",
        "\n",
        "                #Append preprocessed tokens to the list\n",
        "                article_lemmatized_tokens_list.append(article_lemmatized_tokens)\n",
        "\n",
        "                #Text representation using TF-IDF\n",
        "                article_tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(article_lemmatized_tokens)])\n",
        "\n",
        "                #Printing information from the \"fpj_newList\" section\n",
        "                print(f\"Article - Title: {article_title}\")\n",
        "                print(f\"Cleaned Title: {article_title_cleaned}\")\n",
        "                print(f\"Lemmatized Tokens: {article_lemmatized_tokens}\")\n",
        "                print(f\"TF-IDF Matrix: {article_tfidf_matrix}\")\n",
        "                print()\n",
        "\n",
        "                 #Applying K-means clustering\n",
        "                num_clusters = 1  # Adjust based on the desired number of clusters\n",
        "\n",
        "\n",
        "                flattened_tokens_list = [' '.join(tokens) for tokens in article_lemmatized_tokens_list]\n",
        "\n",
        "                 #Using TfidfVectorizer for text vectorization\n",
        "                tfidf_vectorizer_clustering = TfidfVectorizer()\n",
        "                tfidf_matrix_clustering = tfidf_vectorizer_clustering.fit_transform(flattened_tokens_list)\n",
        "\n",
        "                 #Applying K-means clustering\n",
        "                kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "                cluster_labels = kmeans.fit_predict(tfidf_matrix_clustering)\n",
        "\n",
        "                 #Print cluster labels\n",
        "                print(\"Cluster Labels:\", cluster_labels)\n",
        "\n",
        "\n",
        "            #Data for classification model\n",
        "            data = {'text': [' '.join(tokens) for tokens in article_lemmatized_tokens_list], 'cluster_label': cluster_labels}\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            #Split data for training and testing\n",
        "            X_train, X_test, y_train, y_test = train_test_split(df['text'], df['cluster_label'], test_size=0.2, random_state=42)\n",
        "\n",
        "            #Text Vectorization using TF-IDF\n",
        "            tfidf_vectorizer = TfidfVectorizer()\n",
        "            X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "            X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "            #Training Naive Bayes model\n",
        "            nb_model = MultinomialNB()\n",
        "            nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "            #Predictions\n",
        "            y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "            #Evaluate the model\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(\"Accuracy:\", accuracy)\n",
        "            print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "        else:\n",
        "            print(\"New list section not found on the page.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "website_link = 'https://www.freepressjournal.in'\n",
        "extract_information(website_link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tchpw3e_bVx4",
        "outputId": "6be31f92-092e-47ce-bd76-8aef27c2af7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIG NEWS - Title: Adani-Hindenburg Row: Supreme Court To Pronounce Verdict Tomorrow On Petitions Seeking Probe\n",
            "Cleaned Title: Supreme Court To Pronounce Verdict Tomorrow On Petitions Seeking Probe\n",
            "Lemmatized Tokens: ['Supreme', 'Court', 'Pronounce', 'Verdict', 'Tomorrow', 'Petitions', 'Seeking', 'Probe']\n",
            "TF-IDF Matrix:   (0, 2)\t0.35355339059327373\n",
            "  (0, 4)\t0.35355339059327373\n",
            "  (0, 1)\t0.35355339059327373\n",
            "  (0, 6)\t0.35355339059327373\n",
            "  (0, 7)\t0.35355339059327373\n",
            "  (0, 3)\t0.35355339059327373\n",
            "  (0, 0)\t0.35355339059327373\n",
            "  (0, 5)\t0.35355339059327373\n",
            "\n",
            "Article - Title: Mumbai News: Cityflo Bus Services Paused For Wednesday Amid Truck Drivers' Protest\n",
            "Cleaned Title: Mumbai Cityflo Bus Services Paused For Wednesday Amid Truck Protest\n",
            "Lemmatized Tokens: ['Mumbai', 'Cityflo', 'Bus', 'Services', 'Paused', 'Wednesday', 'Amid', 'Truck', 'Protest']\n",
            "TF-IDF Matrix:   (0, 5)\t0.3333333333333333\n",
            "  (0, 7)\t0.3333333333333333\n",
            "  (0, 0)\t0.3333333333333333\n",
            "  (0, 8)\t0.3333333333333333\n",
            "  (0, 4)\t0.3333333333333333\n",
            "  (0, 6)\t0.3333333333333333\n",
            "  (0, 1)\t0.3333333333333333\n",
            "  (0, 2)\t0.3333333333333333\n",
            "  (0, 3)\t0.3333333333333333\n",
            "\n",
            "Cluster Labels: [0]\n",
            "Article - Title: 'First' Rape In Metaverse: Teenage Girl's Avatar 'Sexually Attacked' By Men In Virtual Reality Game In UK, Probe Launched\n",
            "Cleaned Title: Rape In Teenage Avatar By Men In Virtual Reality Game In Probe Launched\n",
            "Lemmatized Tokens: ['Rape', 'Teenage', 'Avatar', 'Men', 'Virtual', 'Reality', 'Game', 'Probe', 'Launched']\n",
            "TF-IDF Matrix:   (0, 2)\t0.3333333333333333\n",
            "  (0, 4)\t0.3333333333333333\n",
            "  (0, 1)\t0.3333333333333333\n",
            "  (0, 6)\t0.3333333333333333\n",
            "  (0, 8)\t0.3333333333333333\n",
            "  (0, 3)\t0.3333333333333333\n",
            "  (0, 0)\t0.3333333333333333\n",
            "  (0, 7)\t0.3333333333333333\n",
            "  (0, 5)\t0.3333333333333333\n",
            "\n",
            "Cluster Labels: [0 0]\n",
            "Article - Title: Viral Video: Tyre Puncture Leads To Collision Of Cars On Duddukuru Highway In Andhra Pradesh, 19-Month-Old Among 3 Dead\n",
            "Cleaned Title: Viral Tyre Puncture Leads To Collision Of Cars On Duddukuru Highway In Andhra Among Dead\n",
            "Lemmatized Tokens: ['Viral', 'Tyre', 'Puncture', 'Leads', 'Collision', 'Cars', 'Duddukuru', 'Highway', 'Andhra', 'Among', 'Dead']\n",
            "TF-IDF Matrix:   (0, 4)\t0.30151134457776363\n",
            "  (0, 0)\t0.30151134457776363\n",
            "  (0, 1)\t0.30151134457776363\n",
            "  (0, 6)\t0.30151134457776363\n",
            "  (0, 5)\t0.30151134457776363\n",
            "  (0, 2)\t0.30151134457776363\n",
            "  (0, 3)\t0.30151134457776363\n",
            "  (0, 7)\t0.30151134457776363\n",
            "  (0, 8)\t0.30151134457776363\n",
            "  (0, 9)\t0.30151134457776363\n",
            "  (0, 10)\t0.30151134457776363\n",
            "\n",
            "Cluster Labels: [0 0 0]\n",
            "Article - Title: 'Progress, Everyday': Agastya Joins Dad Hardik Pandya In The Gym As He Prepares For India Comeback; Watch\n",
            "Cleaned Title: Agastya Joins Dad Hardik Pandya In The Gym As He Prepares For India Watch\n",
            "Lemmatized Tokens: ['Agastya', 'Joins', 'Dad', 'Hardik', 'Pandya', 'Gym', 'Prepares', 'India', 'Watch']\n",
            "TF-IDF Matrix:   (0, 8)\t0.3333333333333333\n",
            "  (0, 4)\t0.3333333333333333\n",
            "  (0, 7)\t0.3333333333333333\n",
            "  (0, 2)\t0.3333333333333333\n",
            "  (0, 6)\t0.3333333333333333\n",
            "  (0, 3)\t0.3333333333333333\n",
            "  (0, 1)\t0.3333333333333333\n",
            "  (0, 5)\t0.3333333333333333\n",
            "  (0, 0)\t0.3333333333333333\n",
            "\n",
            "Cluster Labels: [0 0 0 0]\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***The project successfully achieved automated clustering and classification of news articles, allowing for quick and efficient organization of diverse news topics. The script provided insights into the content structure of the website, and the classification model demonstrated high accuracy in categorizing articles.***"
      ],
      "metadata": {
        "id": "jLJXuTod5HHh"
      }
    }
  ]
}